{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54b0b6f1-02d9-4e50-8ab8-8ebdeccf5653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.llms import CTransformers\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from fastapi import FastAPI, Request, Form, Response\n",
    "from fastapi.responses import HTMLResponse\n",
    "from fastapi.templating import Jinja2Templates\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from fastapi.encoders import jsonable_encoder\n",
    "from qdrant_client import QdrantClient\n",
    "from langchain.vectorstores import Qdrant\n",
    "import os\n",
    "import json\n",
    "\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "from langchain.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2f5e8bde-19df-4bfa-b731-030acb9eb53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelResponse(id='chatcmpl-95fd2076-f873-4e04-9f58-e4df3b3c611f', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Cancer is a group of diseases characterized by the uncontrolled growth and spread of abnormal cells in the body. These cells can invade and destroy nearby normal tissue, organs, and other structures, and can also spread (metastasize) to other parts of the body through the bloodstream or lymphatic system. Cancer is not a single disease but rather a complex group of diseases with many different causes, types of cells affected, and methods of progression. The exact mechanisms that initiate and promote cancer are still being studied by scientists and researchers around the world.', role='assistant'))], created=1702504097, model='ollama/zephyr', object='chat.completion', system_fingerprint=None, usage=Usage(prompt_tokens=4, completion_tokens=110, total_tokens=114), _response_ms=4063.4750000000004)\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "\n",
    "response = completion(\n",
    "    model=\"ollama/zephyr\", \n",
    "    messages=[{ \"content\": \"what is cancer?\",\"role\": \"user\"}], \n",
    "    api_base=\"http://localhost:11434\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9094e55d-f9a4-4a92-88ca-2d9dc5e994ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f872350a-2dfc-4184-80e5-e0bb05a8af36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting async_generator\n",
      "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: async_generator\n",
      "Successfully installed async_generator-1.10\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install async_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeaf39e-b7bd-4780-aefc-ed097f2f79ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d68a6d2-1c50-47af-bfcf-50fb10f667db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile script.py\n",
    "\n",
    "from litellm import completion\n",
    "import litellm\n",
    "async def async_ollama():\n",
    "    response = await litellm.acompletion(\n",
    "        model=\"ollama/meditron\", \n",
    "        messages=[{ \"content\": \"what is cancer?\" ,\"role\": \"user\"}], \n",
    "        api_base=\"http://localhost:11434\", \n",
    "        stream=True\n",
    "    )\n",
    "    async for chunk in response:\n",
    "        print(chunk)\n",
    "\n",
    "# call async_ollama\n",
    "import asyncio\n",
    "asyncio.run(async_ollama())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa777722-9d5d-43bc-bc44-d4a31950280f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/home/otmane_elbourki/my-workspace-26-11-2023/projects/medical-rag-meditron7b/script.py:16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# call async_ollama\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01masyncio\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43masync_ollama\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/ml/lib/python3.10/asyncio/runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "\n",
    "%run script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c74cf6d0-f72f-41d1-9874-c1b6336fe3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatLiteLLM\n",
    "from langchain.schema import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c5175af7-bc5e-4715-bd9e-09ca0dcf5fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatLiteLLM(\n",
    "     model=\"ollama/zephyr\", \n",
    "    api_base=\"http://localhost:11434\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4788d20b-7562-4868-ad28-61aeb390626e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatLiteLLM(client=<module 'litellm' from '/opt/conda/envs/ml/lib/python3.10/site-packages/litellm/__init__.py'>, model='ollama/zephyr', openai_api_key='', azure_api_key='', anthropic_api_key='', replicate_api_key='', cohere_api_key='', openrouter_api_key='', api_base='http://localhost:11434', huggingface_api_key='', together_ai_api_key='')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9ae4395e-54c6-4b65-a0f3-0e3ecc88872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Use the following pieces of information to answer the user's question.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Only return the helpful answer below and nothing else.\n",
    "Helpful answer:\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "556b2e6e-5cbf-4378-ad6c-2c521d29602d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = SentenceTransformerEmbeddings(model_name=\"NeuML/pubmedbert-base-embeddings\")\n",
    "\n",
    "url = \"http://localhost:6333\"\n",
    "\n",
    "client = QdrantClient(\n",
    "    url=url, prefer_grpc=False\n",
    ")\n",
    "\n",
    "db = Qdrant(client=client, embeddings=embeddings, collection_name=\"vector_db\")\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=['context', 'question'])\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\":1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "37ece9bb-1451-4eb1-a498-01dbcd22de00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], template=\"Use the following pieces of information to answer the user's question.\\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\nContext: {context}\\nQuestion: {question}\\n\\nOnly return the helpful answer below and nothing else.\\nHelpful answer:\")"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cbfb7f8a-bb02-4ae7-99ac-3762992f7877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is cancer?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'query': 'what is cancer?', 'result': 'Cancer is the uncontrolled growth of abnormal cells in any part of the body. These cells have lost their ability to stop dividing, leading to the formation of a mass or lump called a tumor or lesion. Cancer can arise from any organ or structure and may not be detectable until it reaches a size of 1 cm or contains 1 million cells. Exceptions include blood and bone marrow cancers, such as leukemias and lymphomas, which may not produce a mass but are evident through laboratory tests. Cancer arises from the inability of immune cells to identify and destroy these newly formed cancerous cells.', 'source_documents': [Document(page_content='What is cancer? Put simply; cancer is the abnormal growth \\nof cells. Cancers arise from any organ or body structure and \\nare composed of tiny cells that have lost the ability to stop \\ngrowing. Occasionally , cancer may be detected “incidentally” by \\na laboratory test or radiological routine test or for an entirely \\ndifferent reason. In general, cancer must reach a size of 1 \\ncm, or be comprised of 1 million cells, before it is detected. \\nAt this point, it may be referred to as a “‘mass,” a “growth,” \\na “tumor,” a “nodule,” a “lump,” or a “lesion.” Exceptions \\nto this general rule include cancers of the blood and bone \\nmarrow (leukemia’s and lymphomas) – which frequently do \\nnot produce a “mass,” but will be evident on laboratory tests.\\nTransformation of a normal cell into a cancerous cell is \\nprobably not such a critical event in the genesis of cancer; \\nrather it is the inability of immune cells of the body to \\nidentify and destroy the newly formed cancer cells when', metadata={'page': 0, 'source': 'data/cancer_and_cure__a_critical_analysis.27.pdf'})]}\n"
     ]
    }
   ],
   "source": [
    "query = \"what is cancer?\"\n",
    "chain_type_kwargs = {\"prompt\": prompt}\n",
    "qa = RetrievalQA.from_chain_type(llm=chat, chain_type=\"stuff\", retriever=retriever, return_source_documents=True, chain_type_kwargs=chain_type_kwargs, verbose=True)\n",
    "print(query)\n",
    "response = qa(query)\n",
    "print(response)\n",
    "answer = response['result']\n",
    "source_document = response['source_documents'][0].page_content\n",
    "doc = response['source_documents'][0].metadata['source']\n",
    "# response_data = jsonable_encoder(json.dumps({\"answer\": answer, \"source_document\": source_document, \"doc\": doc}))\n",
    "\n",
    "# res = Response(response_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "12ae1f00-fd65-47d9-b33b-1ff6c7ce93bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b8d2d3e5-6aa0-496b-8b26-200233311d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(prompt):\n",
    "    # if args.shots > 0:\n",
    "    #     prompt = prompt[:-1]\n",
    "    # if \"orca\" in args.checkpoint_name:\n",
    "    #     system_msg = \"You are an AI assistant who helps people find information.\"\n",
    "    #     return f\"<|im_start|> system\\n{system_msg}<|im_end|>\\n<|im_start|> question\\n{prompt}<|im_end|>\\n<|im_start|> answer\\n\"\n",
    "    system_msg = \"You are a helpful, respectful and honest assistant.\" + \\\n",
    "    \"Always answer as helpfully as possible, while being safe.\" + \\\n",
    "    \"Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\" + \\\n",
    "    \"Please ensure that your responses are socially unbiased and positive in nature.\\n\\n\" + \\\n",
    "    \"If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\" + \\\n",
    "    \"If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "    return f\"<|im_start|> system\\n{system_msg}<|im_end|>\\n <|im_start|> user\\n{prompt}<|im_end|>\\n <|im_start|> assistant\\n\"\n",
    "pr = format_prompt(\"what is covide-19?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a46d0f24-b512-45b6-acc4-b48d850e0d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg = \"You are a helpful, respectful and honest assistant.\" + \\\n",
    "\"Always answer as helpfully as possible, while being safe.\" + \\\n",
    "\"Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.\" + \\\n",
    "\"Please ensure that your responses are socially unbiased and positive in nature.\\n\\n\" + \\\n",
    "\"If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.\" + \\\n",
    "\"If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "prompt1 = 'what happens if listreia is left untreated?'\n",
    "temp = f\"<|im_start|> system\\n{system_msg}<|im_end|>\\n <|im_start|> user\\n{prompt1}<|im_end|>\\n <|im_start|> assistant\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "527d4b5e-8391-4be5-a14d-eeca72ed69ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|> system\n",
      "You are a helpful, respectful and honest assistant.Always answer as helpfully as possible, while being safe.Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.Please ensure that your responses are socially unbiased and positive in nature.\n",
      "\n",
      "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.If you don't know the answer to a question, please don't share false information.<|im_end|>\n",
      " <|im_start|> user\n",
      "what happens if listreia is left untreated?<|im_end|>\n",
      " <|im_start|> assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555f7216-0153-4438-a5b4-46c796409d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "<|im_start|>system\n",
    "{system_msg}<|im_end|>\n",
    "<|im_start|>user\n",
    "{prompt}\n",
    "<|im_end|>\n",
    "<|im_start|>assistant\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "527e8c74-e796-423b-adb5-9b9fb0825906",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=temp, input_variables=['prompt1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2932d2f6-87c1-4586-b8e7-1466b711da66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['prompt1'], template=\"<|im_start|> system\\nYou are a helpful, respectful and honest assistant.Always answer as helpfully as possible, while being safe.Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct.If you don't know the answer to a question, please don't share false information.<|im_end|>\\n <|im_start|> user\\n{prompt1}<|im_end|>\\n <|im_start|> assistant\\n\")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e9d63cd4-337a-4e37-85fa-7122ad111f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "732023d3-f756-4e9c-99d3-3b3c35b9774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'what happens if listreia is left untreated?'\n",
    "lres = llm_chain.run({'prompt1':name})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e0dcfe26-5c46-48f5-8509-e808a6c40904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "64e81e49-55e2-4f0e-be57-33fe86861e9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Got unknown type w",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhat is cancer?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/ml/lib/python3.10/site-packages/langchain/chat_models/base.py:349\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    348\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 349\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    350\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    351\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    353\u001b[0m ]\n\u001b[1;32m    354\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/opt/conda/envs/ml/lib/python3.10/site-packages/langchain/chat_models/base.py:339\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 339\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m         )\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/conda/envs/ml/lib/python3.10/site-packages/langchain/chat_models/base.py:492\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    489\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    490\u001b[0m     )\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported:\n\u001b[0;32m--> 492\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/envs/ml/lib/python3.10/site-packages/langchain/chat_models/litellm.py:304\u001b[0m, in \u001b[0;36mChatLiteLLM._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m     stream_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream(\n\u001b[1;32m    300\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    301\u001b[0m     )\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _generate_from_stream(stream_iter)\n\u001b[0;32m--> 304\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_message_dicts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m    306\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletion_with_retry(\n\u001b[1;32m    307\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessage_dicts, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m    308\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/ml/lib/python3.10/site-packages/langchain/chat_models/litellm.py:335\u001b[0m, in \u001b[0;36mChatLiteLLM._create_message_dicts\u001b[0;34m(self, messages, stop)\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`stop` found in both the input and default params.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    334\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m stop\n\u001b[0;32m--> 335\u001b[0m message_dicts \u001b[38;5;241m=\u001b[39m [_convert_message_to_dict(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m message_dicts, params\n",
      "File \u001b[0;32m/opt/conda/envs/ml/lib/python3.10/site-packages/langchain/chat_models/litellm.py:335\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`stop` found in both the input and default params.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    334\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m stop\n\u001b[0;32m--> 335\u001b[0m message_dicts \u001b[38;5;241m=\u001b[39m [\u001b[43m_convert_message_to_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m message_dicts, params\n",
      "File \u001b[0;32m/opt/conda/envs/ml/lib/python3.10/site-packages/langchain/chat_models/litellm.py:157\u001b[0m, in \u001b[0;36m_convert_message_to_dict\u001b[0;34m(message)\u001b[0m\n\u001b[1;32m    151\u001b[0m     message_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: message\u001b[38;5;241m.\u001b[39mcontent,\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: message\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    155\u001b[0m     }\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unknown type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m message\u001b[38;5;241m.\u001b[39madditional_kwargs:\n\u001b[1;32m    159\u001b[0m     message_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39madditional_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Got unknown type w"
     ]
    }
   ],
   "source": [
    "chat.generate(\"what is cancer?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733d0b28-1438-4f1a-b153-7772b07eb167",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
